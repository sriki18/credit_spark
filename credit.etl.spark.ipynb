{"cells": [{"metadata": {}, "cell_type": "code", "source": "import ibmos2spark \ncredentials = { 'endpoint': 'https://s3-api.us-geo.objectstorage.service.networklayer.com', 'service_id': 'iam-ServiceId-14644ff0-b5b2-4611-ac1b-3da9fe4a7309', 'iam_service_endpoint': 'https://iam.ng.bluemix.net/oidc/token', 'api_key': 'NbYSF4Tb4Eq7wvtCQL4bGQaHEoJSzEQa_Bu7P9fpudBu' } \nconfiguration_name = 'os_ddb7527012204f00b53a3cb6cde76be3_configs' \ncos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos') \nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate() \ndf = spark.read.format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n.option('header', 'true')\\\n.option('inferSchema', 'true')\\\n.load(cos.url('credit_customer_history.csv', 'default-donotdelete-pr-bnveggkfxwmoac')) \ndf.take(5) ", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20191029165525-0003\nKERNEL_ID = e02be0e4-9044-411f-85a3-02b5274d6424\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 1, "data": {"text/plain": "[Row(ACCOUNT_AGE='up to 1 YR', ACCOUNT_TYPE='above 1000 K USD', CREDIT_HISTORY='EXISTING CREDITS PAID BACK', EMI_TENURE=105, HAS_CO_APPLICANT='NO', HAS_GUARANTOR='YES', IS_DEFAULT='No', IS_STATE_BORDER='YES', IS_URBAN='YES', NUMBER_CREDITS=0, OTHER_INSTALMENT_PLAN='NO', OWN_CAR='YES', OWN_REAL_ESTATE='YES', OWN_RESIDENCE='YES', RFM_SCORE=4, SHIP_INTERNATIONAL='YES', STATE='CT', TRANSACTION_AMOUNT=25788, TRANSACTION_CATEGORY='EDUCATION'),\n Row(ACCOUNT_AGE='up to 1 YR', ACCOUNT_TYPE='UNKNOWN/NONE', CREDIT_HISTORY='EXISTING CREDITS PAID BACK', EMI_TENURE=105, HAS_CO_APPLICANT='NO', HAS_GUARANTOR='NO', IS_DEFAULT='No', IS_STATE_BORDER='NO', IS_URBAN='YES', NUMBER_CREDITS=0, OTHER_INSTALMENT_PLAN='YES', OWN_CAR='YES', OWN_REAL_ESTATE='NO', OWN_RESIDENCE='NO', RFM_SCORE=3, SHIP_INTERNATIONAL='YES', STATE='CT', TRANSACTION_AMOUNT=25788, TRANSACTION_CATEGORY='FURNITURE'),\n Row(ACCOUNT_AGE='up to 1 YR', ACCOUNT_TYPE='UNKNOWN/NONE', CREDIT_HISTORY='EXISTING CREDITS PAID BACK', EMI_TENURE=112, HAS_CO_APPLICANT='NO', HAS_GUARANTOR='NO', IS_DEFAULT='No', IS_STATE_BORDER='YES', IS_URBAN='YES', NUMBER_CREDITS=0, OTHER_INSTALMENT_PLAN='YES', OWN_CAR='YES', OWN_REAL_ESTATE='NO', OWN_RESIDENCE='NO', RFM_SCORE=3, SHIP_INTERNATIONAL='NO', STATE='CT', TRANSACTION_AMOUNT=27630, TRANSACTION_CATEGORY='FURNITURE'),\n Row(ACCOUNT_AGE='above 7 YRS', ACCOUNT_TYPE='above 1000 K USD', CREDIT_HISTORY='ALL CREDITS PAID BACK', EMI_TENURE=140, HAS_CO_APPLICANT='YES', HAS_GUARANTOR='NO', IS_DEFAULT='No', IS_STATE_BORDER='NO', IS_URBAN='YES', NUMBER_CREDITS=0, OTHER_INSTALMENT_PLAN='NO', OWN_CAR='NO', OWN_REAL_ESTATE='NO', OWN_RESIDENCE='NO', RFM_SCORE=3, SHIP_INTERNATIONAL='NO', STATE='PA', TRANSACTION_AMOUNT=31314, TRANSACTION_CATEGORY='USED CAR'),\n Row(ACCOUNT_AGE='up to 1 YR', ACCOUNT_TYPE='above 1000 K USD', CREDIT_HISTORY='EXISTING CREDITS PAID BACK', EMI_TENURE=98, HAS_CO_APPLICANT='YES', HAS_GUARANTOR='YES', IS_DEFAULT='Yes', IS_STATE_BORDER='NO', IS_URBAN='YES', NUMBER_CREDITS=0, OTHER_INSTALMENT_PLAN='YES', OWN_CAR='YES', OWN_REAL_ESTATE='NO', OWN_RESIDENCE='NO', RFM_SCORE=4, SHIP_INTERNATIONAL='NO', STATE='PA', TRANSACTION_AMOUNT=25788, TRANSACTION_CATEGORY='FURNITURE')]"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "# from pyspark.sql import SparkSession\n# spark = SparkSession.builder.getOrCreate()\n# df = spark.read\\\n#   .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n#   .option('header', 'true')\\\n#   .option('inferSchema', 'true')\\\n#   .load(cos.url('credit_customer_history.csv', 'default-donotdelete-pr-bnveggkfxwmoac'))\n# df.take(5)\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## What are the column names?"}, {"metadata": {}, "cell_type": "code", "source": "df.printSchema()", "execution_count": 2, "outputs": [{"output_type": "stream", "text": "root\n |-- ACCOUNT_AGE: string (nullable = true)\n |-- ACCOUNT_TYPE: string (nullable = true)\n |-- CREDIT_HISTORY: string (nullable = true)\n |-- EMI_TENURE: integer (nullable = true)\n |-- HAS_CO_APPLICANT: string (nullable = true)\n |-- HAS_GUARANTOR: string (nullable = true)\n |-- IS_DEFAULT: string (nullable = true)\n |-- IS_STATE_BORDER: string (nullable = true)\n |-- IS_URBAN: string (nullable = true)\n |-- NUMBER_CREDITS: integer (nullable = true)\n |-- OTHER_INSTALMENT_PLAN: string (nullable = true)\n |-- OWN_CAR: string (nullable = true)\n |-- OWN_REAL_ESTATE: string (nullable = true)\n |-- OWN_RESIDENCE: string (nullable = true)\n |-- RFM_SCORE: integer (nullable = true)\n |-- SHIP_INTERNATIONAL: string (nullable = true)\n |-- STATE: string (nullable = true)\n |-- TRANSACTION_AMOUNT: integer (nullable = true)\n |-- TRANSACTION_CATEGORY: string (nullable = true)\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\nimport numpy as np", "execution_count": 3, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "The dataset is available here: https://developer.ibm.com/blogs/snap-ml-use-cases-blog/ , credit default prediction. Quoting the text:\n> The task in this use case is to predict whether a person who has credit will default (not be able to repay his credit). The data scientist is provided with a data set of 10 million transactions, each of which is characterized by 18 features (including account age, account type, credit history, owns car, transaction amount, and transaction category). Also provided are the labels of these transactions, default or not. The task is to build a model to predict whether transactions will default in an unseen data set, that is, a data set that has not been used to train the model and does not have labels."}, {"metadata": {}, "cell_type": "markdown", "source": "## Exploration\n\nFirst create a temporary view to run SQL queries."}, {"metadata": {}, "cell_type": "code", "source": "df.createOrReplaceTempView(\"credit\")", "execution_count": 4, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Feature by feature\n\nLook at each feature, see what values it can be and counts.\nAlso look at classification split of interesting features to see if dataset is balanced."}, {"metadata": {}, "cell_type": "code", "source": "t = spark.sql(\"SELECT DISTINCT ACCOUNT_AGE, COUNT(ACCOUNT_AGE) AS age_counts FROM credit \\\nGROUP BY ACCOUNT_AGE \\\nORDER BY age_counts\")\nt.show()", "execution_count": 5, "outputs": [{"output_type": "stream", "text": "+-----------+----------+\n|ACCOUNT_AGE|age_counts|\n+-----------+----------+\n| 1 to 4 YRS|      1093|\n|        TBD|      1581|\n| 4 to 7 YRS|    141090|\n|above 7 YRS|    165109|\n| up to 1 YR|    691127|\n+-----------+----------+\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "- Many accounts are new.\n- Strangely very few accounts in 1-4 year range (maybe students?)"}, {"metadata": {}, "cell_type": "code", "source": "t = spark.sql(\"SELECT DISTINCT ACCOUNT_TYPE, COUNT(ACCOUNT_TYPE) AS type_counts FROM credit \\\nGROUP BY ACCOUNT_TYPE \\\nORDER BY type_counts\")\nt.show()", "execution_count": 6, "outputs": [{"output_type": "stream", "text": "+-----------------+-----------+\n|     ACCOUNT_TYPE|type_counts|\n+-----------------+-----------+\n|500 to 1000 K USD|       1472|\n|  up to 100 K USD|       2578|\n| 100 to 500 K USD|     141594|\n|     UNKNOWN/NONE|     188476|\n| above 1000 K USD|     665880|\n+-----------------+-----------+\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "- Wow, many transactions from millionaires. Maybe student hypothesis is wrong..."}, {"metadata": {}, "cell_type": "code", "source": "t = spark.sql(\"SELECT DISTINCT ACCOUNT_TYPE, IS_DEFAULT, COUNT(ACCOUNT_TYPE) FROM credit \\\nGROUP BY ACCOUNT_TYPE, IS_DEFAULT \\\nORDER BY ACCOUNT_TYPE, IS_DEFAULT\")\nt.show()", "execution_count": 7, "outputs": [{"output_type": "stream", "text": "+-----------------+----------+-------------------+\n|     ACCOUNT_TYPE|IS_DEFAULT|count(ACCOUNT_TYPE)|\n+-----------------+----------+-------------------+\n| 100 to 500 K USD|        No|              94719|\n| 100 to 500 K USD|       Yes|              46875|\n|500 to 1000 K USD|        No|                667|\n|500 to 1000 K USD|       Yes|                805|\n|     UNKNOWN/NONE|        No|             127634|\n|     UNKNOWN/NONE|       Yes|              60842|\n| above 1000 K USD|        No|             475662|\n| above 1000 K USD|       Yes|             190218|\n|  up to 100 K USD|        No|               1318|\n|  up to 100 K USD|       Yes|               1260|\n+-----------------+----------+-------------------+\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "- Up to 100 K and 500-1000k have 1:1 odds of defaulting:not defaulting, compared to 1:2 of 100-500k and >1000k. "}, {"metadata": {}, "cell_type": "code", "source": "t = spark.sql(\"SELECT DISTINCT CREDIT_HISTORY, COUNT(CREDIT_HISTORY) FROM credit GROUP BY CREDIT_HISTORY\")\nt.show()", "execution_count": 8, "outputs": [{"output_type": "stream", "text": "+--------------------+---------------------+\n|      CREDIT_HISTORY|count(CREDIT_HISTORY)|\n+--------------------+---------------------+\n|          NONE TAKEN|                 7635|\n|ALL CREDITS PAID ...|               456194|\n|       DELAY IN PAST|                61922|\n|EXISTING CREDITS ...|               473511|\n|    CRITICAL ACCOUNT|                  738|\n+--------------------+---------------------+\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "t = spark.sql(\"SELECT IS_DEFAULT, MEAN(EMI_TENURE), STD(EMI_TENURE) FROM credit GROUP BY IS_DEFAULT\")\nt.show()", "execution_count": 9, "outputs": [{"output_type": "stream", "text": "+----------+-----------------+---------------------------------------+\n|IS_DEFAULT|  avg(EMI_TENURE)|stddev_samp(CAST(EMI_TENURE AS DOUBLE))|\n+----------+-----------------+---------------------------------------+\n|        No|        116.77337|                     14.091804445397342|\n|       Yes|93.11754666666667|                     18.168372237132022|\n+----------+-----------------+---------------------------------------+\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "t = spark.sql(\"SELECT MEAN(EMI_TENURE), STD(EMI_TENURE) FROM credit\")\nt.show()", "execution_count": 10, "outputs": [{"output_type": "stream", "text": "+---------------+---------------------------------------+\n|avg(EMI_TENURE)|stddev_samp(CAST(EMI_TENURE AS DOUBLE))|\n+---------------+---------------------------------------+\n|     109.676623|                      18.85596971421701|\n+---------------+---------------------------------------+\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "t = spark.sql(\"SELECT DISTINCT HAS_CO_APPLICANT, COUNT(HAS_CO_APPLICANT) FROM credit GROUP BY HAS_CO_APPLICANT\")\nt.show()", "execution_count": 11, "outputs": [{"output_type": "stream", "text": "+----------------+-----------------------+\n|HAS_CO_APPLICANT|count(HAS_CO_APPLICANT)|\n+----------------+-----------------------+\n|             YES|                 268613|\n|              NO|                 731387|\n+----------------+-----------------------+\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "t = spark.sql(\"SELECT DISTINCT HAS_GUARANTOR, COUNT(HAS_GUARANTOR) FROM credit GROUP BY HAS_GUARANTOR\")\nt.show()", "execution_count": 12, "outputs": [{"output_type": "stream", "text": "+-------------+--------------------+\n|HAS_GUARANTOR|count(HAS_GUARANTOR)|\n+-------------+--------------------+\n|          YES|              397798|\n|           NO|              602202|\n+-------------+--------------------+\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "t = spark.sql(\"SELECT DISTINCT IS_DEFAULT, COUNT(IS_DEFAULT) FROM credit GROUP BY IS_DEFAULT\")\nt.show()", "execution_count": 13, "outputs": [{"output_type": "stream", "text": "+----------+-----------------+\n|IS_DEFAULT|count(IS_DEFAULT)|\n+----------+-----------------+\n|        No|           700000|\n|       Yes|           300000|\n+----------+-----------------+\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "`No` has the higher frequency. Therefore, when using the StringIndexer, `No` will be given the index 0. `Yes` will be given the index 1."}, {"metadata": {}, "cell_type": "code", "source": "t = spark.sql(\"SELECT DISTINCT IS_STATE_BORDER, COUNT(IS_STATE_BORDER) FROM credit GROUP BY IS_STATE_BORDER\")\nt.show()", "execution_count": 14, "outputs": [{"output_type": "stream", "text": "+---------------+----------------------+\n|IS_STATE_BORDER|count(IS_STATE_BORDER)|\n+---------------+----------------------+\n|            YES|                477983|\n|             NO|                522017|\n+---------------+----------------------+\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "t = spark.sql(\"SELECT DISTINCT IS_URBAN, COUNT(IS_URBAN) FROM credit GROUP BY IS_URBAN\")\nt.show()", "execution_count": 15, "outputs": [{"output_type": "stream", "text": "+--------+---------------+\n|IS_URBAN|count(IS_URBAN)|\n+--------+---------------+\n|     YES|         623097|\n|      NO|         376903|\n+--------+---------------+\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "t = spark.sql(\"SELECT DISTINCT NUMBER_CREDITS, COUNT(NUMBER_CREDITS) FROM credit GROUP BY NUMBER_CREDITS\")\nt.show()", "execution_count": 16, "outputs": [{"output_type": "stream", "text": "+--------------+---------------------+\n|NUMBER_CREDITS|count(NUMBER_CREDITS)|\n+--------------+---------------------+\n|             0|              1000000|\n+--------------+---------------------+\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "No need to include `NUMBER_CREDITS` feature, it is only zeros."}, {"metadata": {}, "cell_type": "code", "source": "t = spark.sql(\"SELECT DISTINCT OTHER_INSTALMENT_PLAN, COUNT(OTHER_INSTALMENT_PLAN) FROM credit GROUP BY OTHER_INSTALMENT_PLAN\")\nt.show()", "execution_count": 17, "outputs": [{"output_type": "stream", "text": "+---------------------+----------------------------+\n|OTHER_INSTALMENT_PLAN|count(OTHER_INSTALMENT_PLAN)|\n+---------------------+----------------------------+\n|                  YES|                      499274|\n|                   NO|                      500726|\n+---------------------+----------------------------+\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "t = spark.sql(\"SELECT DISTINCT OWN_CAR, COUNT(OWN_CAR) FROM credit GROUP BY OWN_CAR\")\nt.show()", "execution_count": 18, "outputs": [{"output_type": "stream", "text": "+-------+--------------+\n|OWN_CAR|count(OWN_CAR)|\n+-------+--------------+\n|    YES|        632143|\n|     NO|        367857|\n+-------+--------------+\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "t = spark.sql(\"SELECT DISTINCT OWN_REAL_ESTATE, COUNT(OWN_REAL_ESTATE) FROM credit GROUP BY OWN_REAL_ESTATE\")\nt.show()", "execution_count": 19, "outputs": [{"output_type": "stream", "text": "+---------------+----------------------+\n|OWN_REAL_ESTATE|count(OWN_REAL_ESTATE)|\n+---------------+----------------------+\n|            YES|                427453|\n|             NO|                572547|\n+---------------+----------------------+\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "t = spark.sql(\"SELECT DISTINCT OWN_RESIDENCE, COUNT(OWN_RESIDENCE) FROM credit GROUP BY OWN_RESIDENCE\")\nt.show()", "execution_count": 20, "outputs": [{"output_type": "stream", "text": "+-------------+--------------------+\n|OWN_RESIDENCE|count(OWN_RESIDENCE)|\n+-------------+--------------------+\n|          YES|              473235|\n|           NO|              526765|\n+-------------+--------------------+\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "t = spark.sql(\"SELECT DISTINCT RFM_SCORE, COUNT(RFM_SCORE) FROM credit GROUP BY RFM_SCORE ORDER BY RFM_SCORE\")\nt.show()", "execution_count": 21, "outputs": [{"output_type": "stream", "text": "+---------+----------------+\n|RFM_SCORE|count(RFM_SCORE)|\n+---------+----------------+\n|        1|           11460|\n|        2|            3375|\n|        3|          407275|\n|        4|          577890|\n+---------+----------------+\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Only four different scores, but there is intrinsic ordering. So either use as-is or consider clubbing 1&2 and using as categorical."}, {"metadata": {}, "cell_type": "code", "source": "t = spark.sql(\"SELECT DISTINCT SHIP_INTERNATIONAL, IS_DEFAULT, COUNT(SHIP_INTERNATIONAL) FROM credit \\\nGROUP BY SHIP_INTERNATIONAL, IS_DEFAULT \\\nORDER BY SHIP_INTERNATIONAL, IS_DEFAULT\")\nt.show()", "execution_count": 22, "outputs": [{"output_type": "stream", "text": "+------------------+----------+-------------------------+\n|SHIP_INTERNATIONAL|IS_DEFAULT|count(SHIP_INTERNATIONAL)|\n+------------------+----------+-------------------------+\n|                NO|        No|                   333691|\n|                NO|       Yes|                   142762|\n|               YES|        No|                   366309|\n|               YES|       Yes|                   157238|\n+------------------+----------+-------------------------+\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "International or not probably has little to do with default directly. May have something to do with transaction amount?"}, {"metadata": {}, "cell_type": "code", "source": "t = spark.sql(\"SELECT DISTINCT SHIP_INTERNATIONAL, COUNT(SHIP_INTERNATIONAL), MEAN(TRANSACTION_AMOUNT), STD(TRANSACTION_AMOUNT) FROM credit \\\nGROUP BY SHIP_INTERNATIONAL \\\nORDER BY SHIP_INTERNATIONAL\")\nt.show()", "execution_count": 23, "outputs": [{"output_type": "stream", "text": "+------------------+-------------------------+-----------------------+-----------------------------------------------+\n|SHIP_INTERNATIONAL|count(SHIP_INTERNATIONAL)|avg(TRANSACTION_AMOUNT)|stddev_samp(CAST(TRANSACTION_AMOUNT AS DOUBLE))|\n+------------------+-------------------------+-----------------------+-----------------------------------------------+\n|                NO|                   476453|     26789.872356769712|                              5102.042903503083|\n|               YES|                   523547|     27153.019362158506|                              4098.496932157878|\n+------------------+-------------------------+-----------------------+-----------------------------------------------+\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "International shipping or not has little to do with transaction amount."}, {"metadata": {}, "cell_type": "code", "source": "t = spark.sql(\"SELECT DISTINCT STATE, COUNT(STATE) as counts FROM credit GROUP BY STATE ORDER BY counts\")\nt.show()", "execution_count": 24, "outputs": [{"output_type": "stream", "text": "+-----+------+\n|STATE|counts|\n+-----+------+\n|   NJ|  4582|\n|   NY| 13435|\n|   PA|416961|\n|   CT|565022|\n+-----+------+\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "#### Interesting\n- Only 4 states.\n- NJ and NY have 'few' samples percentage-wise. Make as categorical.\n- Also the 4 states border each other. May want to look at other features such as population/GDP just to see what it's like."}, {"metadata": {}, "cell_type": "code", "source": "t = spark.sql(\"SELECT MEAN(TRANSACTION_AMOUNT), STD(TRANSACTION_AMOUNT) FROM credit GROUP BY IS_DEFAULT\")\nt.show()", "execution_count": 25, "outputs": [{"output_type": "stream", "text": "+-----------------------+-----------------------------------------------+\n|avg(TRANSACTION_AMOUNT)|stddev_samp(CAST(TRANSACTION_AMOUNT AS DOUBLE))|\n+-----------------------+-----------------------------------------------+\n|      28820.70300857143|                             3243.9050912348657|\n|            22685.01592|                              4456.413935434743|\n+-----------------------+-----------------------------------------------+\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "t = spark.sql(\"SELECT TRANSACTION_CATEGORY, COUNT(TRANSACTION_CATEGORY) as counts FROM credit\\\n              GROUP BY TRANSACTION_CATEGORY \\\n              ORDER BY counts\")\nt.show()", "execution_count": 26, "outputs": [{"output_type": "stream", "text": "+--------------------+------+\n|TRANSACTION_CATEGORY|counts|\n+--------------------+------+\n|               OTHER|   686|\n|             NEW CAR|   762|\n|          RETRAINING| 27271|\n|            USED CAR| 41880|\n|           EDUCATION|186619|\n|         ELECTRONICS|316553|\n|           FURNITURE|426229|\n+--------------------+------+\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "#### Interesting\n\n- Most of the transactions are for Furniture, which is quite unexpected. It may be a biased dataset, but intersting nevertheless.\n- And cars (new/used) form a 10th of the furniture transactions."}, {"metadata": {}, "cell_type": "code", "source": "t = spark.sql(\"SELECT DISTINCT TRANSACTION_CATEGORY, IS_DEFAULT, COUNT(TRANSACTION_CATEGORY) FROM credit \\\nGROUP BY TRANSACTION_CATEGORY, IS_DEFAULT ORDER BY TRANSACTION_CATEGORY, IS_DEFAULT\")\nt.show()", "execution_count": 27, "outputs": [{"output_type": "stream", "text": "+--------------------+----------+---------------------------+\n|TRANSACTION_CATEGORY|IS_DEFAULT|count(TRANSACTION_CATEGORY)|\n+--------------------+----------+---------------------------+\n|           EDUCATION|        No|                      45615|\n|           EDUCATION|       Yes|                     141004|\n|         ELECTRONICS|        No|                     301451|\n|         ELECTRONICS|       Yes|                      15102|\n|           FURNITURE|        No|                     309836|\n|           FURNITURE|       Yes|                     116393|\n|             NEW CAR|        No|                        758|\n|             NEW CAR|       Yes|                          4|\n|               OTHER|        No|                          4|\n|               OTHER|       Yes|                        682|\n|          RETRAINING|        No|                        914|\n|          RETRAINING|       Yes|                      26357|\n|            USED CAR|        No|                      41422|\n|            USED CAR|       Yes|                        458|\n+--------------------+----------+---------------------------+\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "#### Interesting\n- Education, retraining and 'other' category more likely to default than not. \n- Odds of defaulting:not defaulting for Electronics is 1:20, Furniture is 1:3, used car is 1:100."}, {"metadata": {}, "cell_type": "markdown", "source": "# Loading\n\n- Transform the data into categorical or centered numeric features for classification.\n\n## Tools\n- `VectorAssembler` - makes a column of numerical vectors from input columns\n- `StandardScaler` - make mean = 0, std = 1\n- `StringIndexer` - convert strings to categorical variables with lower index representing more frequent sample in dataset\n- `OneHotEncoder` - converts categorical variables to one-hot encoding\n\nFirst assemble the numerical values into a vector and scale them."}, {"metadata": {}, "cell_type": "code", "source": "va1 = VectorAssembler(inputCols=['RFM_SCORE', 'TRANSACTION_AMOUNT', 'EMI_TENURE'], outputCol='rfm_amt_emi')\ndf2 = va1.transform(df)\nss = StandardScaler(withMean=True, inputCol='rfm_amt_emi', outputCol='rfm_amt_emi_scaled')\nssmodel = ss.fit(df2)\ndf2 = ssmodel.transform(df2)\nprint(ssmodel.mean, ssmodel.std)", "execution_count": 28, "outputs": [{"output_type": "stream", "text": "[3.5515949999999714,26979.996881999785,109.67662299999911] [0.5681973942418947,4607.57206512009,18.855969714217142]\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "#### Verify standard scaler does what is expected\n\nScaling again should yield a mean of 0 and std dev of 1."}, {"metadata": {}, "cell_type": "code", "source": "ss2 = StandardScaler(inputCol=\"rfm_amt_emi_scaled\", outputCol=\"scaled_twice\")\nssmodel2 = ss2.fit(df2)\nprint(ssmodel2.mean, ssmodel2.std)", "execution_count": 29, "outputs": [{"output_type": "stream", "text": "[5.0237103973310715e-14,4.672222895032996e-14,4.715910821573299e-14] [1.0000000000000004,0.9999999999999951,0.9999999999999921]\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "The work on categorical features. First use `StringIndexer` to convert to categorical. Then for cases with more than 2 categories, use one-hot encoding. Also start assembling feature names."}, {"metadata": {}, "cell_type": "code", "source": "indexer = StringIndexer(inputCol=\"IS_DEFAULT\", outputCol=\"label\")\nndf = indexer.fit(df2).transform(df2)\nnames = [\"ACCOUNT_AGE\", \"ACCOUNT_TYPE\", \"CREDIT_HISTORY\", \n         \"HAS_CO_APPLICANT\", \"HAS_GUARANTOR\", \"IS_STATE_BORDER\", \n         \"IS_URBAN\", \"OTHER_INSTALMENT_PLAN\",\"OWN_CAR\", \n         \"OWN_REAL_ESTATE\", \"OWN_RESIDENCE\", \"SHIP_INTERNATIONAL\", \n         \"STATE\", \"TRANSACTION_CATEGORY\"]\nfeat_names = []\nfeat_base = []\nfor ind, name in enumerate(names):\n    inpcol = name\n    oupcol = name.lower() + \"_ind\"\n    indexer = StringIndexer(inputCol = inpcol, outputCol = oupcol)\n    model = indexer.fit(ndf)\n    ndf = model.transform(ndf)\n    this_labels = model.labels[:-1]\n    [feat_names.append(inpcol + \"(\" + a_label + \")\") for a_label in this_labels]\n    feat_base.append(inpcol + \"(\" + model.labels[-1] + \")\")\n    # last category is not included by default\n    # see docs here: https://spark.apache.org/docs/2.3.0/api/python/pyspark.ml.html#pyspark.ml.feature.OneHotEncoderEstimator\nencoder = OneHotEncoder(inputCol=\"account_age_ind\", outputCol=\"account_age_vec\")\nndf = encoder.transform(ndf)\nencoder = OneHotEncoder(inputCol=\"account_type_ind\", outputCol=\"account_type_vec\")\nndf = encoder.transform(ndf)\nencoder = OneHotEncoder(inputCol=\"credit_history_ind\", outputCol=\"credit_history_vec\")\nndf = encoder.transform(ndf)\nencoder = OneHotEncoder(inputCol=\"transaction_category_ind\", outputCol=\"transaction_vec\")\nndf = encoder.transform(ndf)\nencoder = OneHotEncoder(inputCol=\"state_ind\", outputCol=\"state_vec\")\nndf = encoder.transform(ndf)\n\nassembler = VectorAssembler(\n    inputCols=[\"account_age_vec\", \"account_type_vec\", \"credit_history_vec\",\n               \"has_co_applicant_ind\", \"has_guarantor_ind\", \"is_state_border_ind\", \n               \"is_urban_ind\", \"other_instalment_plan_ind\", \"own_car_ind\", \n               \"own_real_estate_ind\", \"own_residence_ind\", \"ship_international_ind\",\n               \"state_vec\", \"transaction_vec\",\n               \"rfm_amt_emi_scaled\"],\n    outputCol=\"features\")\n\nndf = assembler.transform(ndf)\nndf.take(1)", "execution_count": 30, "outputs": [{"output_type": "execute_result", "execution_count": 30, "data": {"text/plain": "[Row(ACCOUNT_AGE='up to 1 YR', ACCOUNT_TYPE='above 1000 K USD', CREDIT_HISTORY='EXISTING CREDITS PAID BACK', EMI_TENURE=105, HAS_CO_APPLICANT='NO', HAS_GUARANTOR='YES', IS_DEFAULT='No', IS_STATE_BORDER='YES', IS_URBAN='YES', NUMBER_CREDITS=0, OTHER_INSTALMENT_PLAN='NO', OWN_CAR='YES', OWN_REAL_ESTATE='YES', OWN_RESIDENCE='YES', RFM_SCORE=4, SHIP_INTERNATIONAL='YES', STATE='CT', TRANSACTION_AMOUNT=25788, TRANSACTION_CATEGORY='EDUCATION', rfm_amt_emi=DenseVector([4.0, 25788.0, 105.0]), rfm_amt_emi_scaled=DenseVector([0.7892, -0.2587, -0.248]), label=0.0, account_age_ind=0.0, account_type_ind=0.0, credit_history_ind=0.0, has_co_applicant_ind=0.0, has_guarantor_ind=1.0, is_state_border_ind=1.0, is_urban_ind=0.0, other_instalment_plan_ind=0.0, own_car_ind=0.0, own_real_estate_ind=1.0, own_residence_ind=1.0, ship_international_ind=0.0, state_ind=0.0, transaction_category_ind=2.0, account_age_vec=SparseVector(4, {0: 1.0}), account_type_vec=SparseVector(4, {0: 1.0}), credit_history_vec=SparseVector(4, {0: 1.0}), transaction_vec=SparseVector(6, {2: 1.0}), state_vec=SparseVector(3, {0: 1.0}), features=SparseVector(33, {0: 1.0, 4: 1.0, 8: 1.0, 13: 1.0, 14: 1.0, 18: 1.0, 19: 1.0, 21: 1.0, 26: 1.0, 30: 0.7892, 31: -0.2587, 32: -0.248}))]"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Get feature names\n\n- For the entire input vector\n- For the \"base\" case, where all categorical indices are 0 and numerical features are set to mean values."}, {"metadata": {}, "cell_type": "code", "source": "feat_names_full = feat_names + [\"scaled rfm\", \"scaled transaction amount\", \"scaled emi tenure\"]\nfeat_base_full = feat_base + [\"rfm = \" + str(round(ssmodel.mean[0], 2)), \n                              \"transaction amount = \" + str(round(ssmodel.mean[1], 2)),\n                              \"emi tenure = \" + str(round(ssmodel.mean[2], 2))]\nprint(feat_base_full)", "execution_count": 31, "outputs": [{"output_type": "stream", "text": "['ACCOUNT_AGE(1 to 4 YRS)', 'ACCOUNT_TYPE(500 to 1000 K USD)', 'CREDIT_HISTORY(CRITICAL ACCOUNT)', 'HAS_CO_APPLICANT(YES)', 'HAS_GUARANTOR(YES)', 'IS_STATE_BORDER(YES)', 'IS_URBAN(NO)', 'OTHER_INSTALMENT_PLAN(YES)', 'OWN_CAR(NO)', 'OWN_REAL_ESTATE(YES)', 'OWN_RESIDENCE(YES)', 'SHIP_INTERNATIONAL(NO)', 'STATE(NJ)', 'TRANSACTION_CATEGORY(OTHER)', 'rfm = 3.55', 'transaction amount = 26980.0', 'emi tenure = 109.68']\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "### Save features and transformed data"}, {"metadata": {}, "cell_type": "code", "source": "np.savez(\"feat_names.npz\", feat_base_full=feat_base_full, feat_names_full=feat_names_full,\n        mean=ssmodel.mean.toArray(), std=ssmodel.std.toArray())", "execution_count": 32, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "ndf.select(\"label\", \"features\").write.mode('overwrite').save(\"credit_feats.parquet\")", "execution_count": 33, "outputs": []}], "metadata": {"kernelspec": {"name": "python36", "display_name": "Python 3.6 with Spark", "language": "python3"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "pygments_lexer": "ipython3", "version": "3.6.8", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat": 4, "nbformat_minor": 1}