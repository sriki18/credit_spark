{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ACCOUNT_AGE=u'up to 1 YR', ACCOUNT_TYPE=u'above 1000 K USD', CREDIT_HISTORY=u'EXISTING CREDITS PAID BACK', EMI_TENURE=105, HAS_CO_APPLICANT=u'NO', HAS_GUARANTOR=u'YES', IS_DEFAULT=u'No', IS_STATE_BORDER=u'YES', IS_URBAN=u'YES', NUMBER_CREDITS=0, OTHER_INSTALMENT_PLAN=u'NO', OWN_CAR=u'YES', OWN_REAL_ESTATE=u'YES', OWN_RESIDENCE=u'YES', RFM_SCORE=4, SHIP_INTERNATIONAL=u'YES', STATE=u'CT', TRANSACTION_AMOUNT=25788, TRANSACTION_CATEGORY=u'EDUCATION'),\n",
       " Row(ACCOUNT_AGE=u'up to 1 YR', ACCOUNT_TYPE=u'UNKNOWN/NONE', CREDIT_HISTORY=u'EXISTING CREDITS PAID BACK', EMI_TENURE=105, HAS_CO_APPLICANT=u'NO', HAS_GUARANTOR=u'NO', IS_DEFAULT=u'No', IS_STATE_BORDER=u'NO', IS_URBAN=u'YES', NUMBER_CREDITS=0, OTHER_INSTALMENT_PLAN=u'YES', OWN_CAR=u'YES', OWN_REAL_ESTATE=u'NO', OWN_RESIDENCE=u'NO', RFM_SCORE=3, SHIP_INTERNATIONAL=u'YES', STATE=u'CT', TRANSACTION_AMOUNT=25788, TRANSACTION_CATEGORY=u'FURNITURE'),\n",
       " Row(ACCOUNT_AGE=u'up to 1 YR', ACCOUNT_TYPE=u'UNKNOWN/NONE', CREDIT_HISTORY=u'EXISTING CREDITS PAID BACK', EMI_TENURE=112, HAS_CO_APPLICANT=u'NO', HAS_GUARANTOR=u'NO', IS_DEFAULT=u'No', IS_STATE_BORDER=u'YES', IS_URBAN=u'YES', NUMBER_CREDITS=0, OTHER_INSTALMENT_PLAN=u'YES', OWN_CAR=u'YES', OWN_REAL_ESTATE=u'NO', OWN_RESIDENCE=u'NO', RFM_SCORE=3, SHIP_INTERNATIONAL=u'NO', STATE=u'CT', TRANSACTION_AMOUNT=27630, TRANSACTION_CATEGORY=u'FURNITURE'),\n",
       " Row(ACCOUNT_AGE=u'above 7 YRS', ACCOUNT_TYPE=u'above 1000 K USD', CREDIT_HISTORY=u'ALL CREDITS PAID BACK', EMI_TENURE=140, HAS_CO_APPLICANT=u'YES', HAS_GUARANTOR=u'NO', IS_DEFAULT=u'No', IS_STATE_BORDER=u'NO', IS_URBAN=u'YES', NUMBER_CREDITS=0, OTHER_INSTALMENT_PLAN=u'NO', OWN_CAR=u'NO', OWN_REAL_ESTATE=u'NO', OWN_RESIDENCE=u'NO', RFM_SCORE=3, SHIP_INTERNATIONAL=u'NO', STATE=u'PA', TRANSACTION_AMOUNT=31314, TRANSACTION_CATEGORY=u'USED CAR'),\n",
       " Row(ACCOUNT_AGE=u'up to 1 YR', ACCOUNT_TYPE=u'above 1000 K USD', CREDIT_HISTORY=u'EXISTING CREDITS PAID BACK', EMI_TENURE=98, HAS_CO_APPLICANT=u'YES', HAS_GUARANTOR=u'YES', IS_DEFAULT=u'Yes', IS_STATE_BORDER=u'NO', IS_URBAN=u'YES', NUMBER_CREDITS=0, OTHER_INSTALMENT_PLAN=u'YES', OWN_CAR=u'YES', OWN_REAL_ESTATE=u'NO', OWN_RESIDENCE=u'NO', RFM_SCORE=4, SHIP_INTERNATIONAL=u'NO', STATE=u'PA', TRANSACTION_AMOUNT=25788, TRANSACTION_CATEGORY=u'FURNITURE')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df = spark.read\\\n",
    "  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n",
    "  .option('header', 'true')\\\n",
    "  .option('inferSchema', 'true')\\\n",
    "  .load(cos.url('credit_customer_history.csv', 'default-donotdelete-pr-bnveggkfxwmoac'))\n",
    "df.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ACCOUNT_AGE: string (nullable = true)\n",
      " |-- ACCOUNT_TYPE: string (nullable = true)\n",
      " |-- CREDIT_HISTORY: string (nullable = true)\n",
      " |-- EMI_TENURE: integer (nullable = true)\n",
      " |-- HAS_CO_APPLICANT: string (nullable = true)\n",
      " |-- HAS_GUARANTOR: string (nullable = true)\n",
      " |-- IS_DEFAULT: string (nullable = true)\n",
      " |-- IS_STATE_BORDER: string (nullable = true)\n",
      " |-- IS_URBAN: string (nullable = true)\n",
      " |-- NUMBER_CREDITS: integer (nullable = true)\n",
      " |-- OTHER_INSTALMENT_PLAN: string (nullable = true)\n",
      " |-- OWN_CAR: string (nullable = true)\n",
      " |-- OWN_REAL_ESTATE: string (nullable = true)\n",
      " |-- OWN_RESIDENCE: string (nullable = true)\n",
      " |-- RFM_SCORE: integer (nullable = true)\n",
      " |-- SHIP_INTERNATIONAL: string (nullable = true)\n",
      " |-- STATE: string (nullable = true)\n",
      " |-- TRANSACTION_AMOUNT: integer (nullable = true)\n",
      " |-- TRANSACTION_CATEGORY: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is available here: https://developer.ibm.com/blogs/snap-ml-use-cases-blog/ , credit default prediction. Quoting the text:\n",
    "> The task in this use case is to predict whether a person who has credit will default (not be able to repay his credit). The data scientist is provided with a data set of 10 million transactions, each of which is characterized by 18 features (including account age, account type, credit history, owns car, transaction amount, and transaction category). Also provided are the labels of these transactions, default or not. The task is to build a model to predict whether transactions will default in an unseen data set, that is, a data set that has not been used to train the model and does not have labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking for NULL or NaN\n",
    "\n",
    "First create a temporary view so that we can run SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"credit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|ACCOUNT_AGE|age_counts|\n",
      "+-----------+----------+\n",
      "| 1 to 4 YRS|      1093|\n",
      "|        TBD|      1581|\n",
      "| 4 to 7 YRS|    141090|\n",
      "|above 7 YRS|    165109|\n",
      "| up to 1 YR|    691127|\n",
      "+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = spark.sql(\"SELECT DISTINCT ACCOUNT_AGE, COUNT(ACCOUNT_AGE) AS age_counts FROM credit \\\n",
    "GROUP BY ACCOUNT_AGE \\\n",
    "ORDER BY age_counts\")\n",
    "t.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+\n",
      "|     ACCOUNT_TYPE|type_counts|\n",
      "+-----------------+-----------+\n",
      "|500 to 1000 K USD|       1472|\n",
      "|  up to 100 K USD|       2578|\n",
      "| 100 to 500 K USD|     141594|\n",
      "|     UNKNOWN/NONE|     188476|\n",
      "| above 1000 K USD|     665880|\n",
      "+-----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = spark.sql(\"SELECT DISTINCT ACCOUNT_TYPE, COUNT(ACCOUNT_TYPE) AS type_counts FROM credit \\\n",
    "GROUP BY ACCOUNT_TYPE \\\n",
    "ORDER BY type_counts\")\n",
    "t.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+-------------------+\n",
      "|     ACCOUNT_TYPE|IS_DEFAULT|count(ACCOUNT_TYPE)|\n",
      "+-----------------+----------+-------------------+\n",
      "| 100 to 500 K USD|        No|              94719|\n",
      "| 100 to 500 K USD|       Yes|              46875|\n",
      "|500 to 1000 K USD|        No|                667|\n",
      "|500 to 1000 K USD|       Yes|                805|\n",
      "|     UNKNOWN/NONE|        No|             127634|\n",
      "|     UNKNOWN/NONE|       Yes|              60842|\n",
      "| above 1000 K USD|        No|             475662|\n",
      "| above 1000 K USD|       Yes|             190218|\n",
      "|  up to 100 K USD|        No|               1318|\n",
      "|  up to 100 K USD|       Yes|               1260|\n",
      "+-----------------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = spark.sql(\"SELECT DISTINCT ACCOUNT_TYPE, IS_DEFAULT, COUNT(ACCOUNT_TYPE) FROM credit \\\n",
    "GROUP BY ACCOUNT_TYPE, IS_DEFAULT \\\n",
    "ORDER BY ACCOUNT_TYPE, IS_DEFAULT\")\n",
    "t.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+\n",
      "|      CREDIT_HISTORY|count(CREDIT_HISTORY)|\n",
      "+--------------------+---------------------+\n",
      "|          NONE TAKEN|                 7635|\n",
      "|ALL CREDITS PAID ...|               456194|\n",
      "|       DELAY IN PAST|                61922|\n",
      "|EXISTING CREDITS ...|               473511|\n",
      "|    CRITICAL ACCOUNT|                  738|\n",
      "+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = spark.sql(\"SELECT DISTINCT CREDIT_HISTORY, COUNT(CREDIT_HISTORY) FROM credit GROUP BY CREDIT_HISTORY\")\n",
    "t.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+---------------------------------------+\n",
      "|IS_DEFAULT|  avg(EMI_TENURE)|stddev_samp(CAST(EMI_TENURE AS DOUBLE))|\n",
      "+----------+-----------------+---------------------------------------+\n",
      "|        No|        116.77337|                     14.091804445397239|\n",
      "|       Yes|93.11754666666667|                     18.168372237131955|\n",
      "+----------+-----------------+---------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = spark.sql(\"SELECT IS_DEFAULT, MEAN(EMI_TENURE), STD(EMI_TENURE) FROM credit GROUP BY IS_DEFAULT\")\n",
    "t.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------------------------------+\n",
      "|avg(EMI_TENURE)|stddev_samp(CAST(EMI_TENURE AS DOUBLE))|\n",
      "+---------------+---------------------------------------+\n",
      "|     109.676623|                     18.855969714216904|\n",
      "+---------------+---------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = spark.sql(\"SELECT MEAN(EMI_TENURE), STD(EMI_TENURE) FROM credit\")\n",
    "t.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------------+\n",
      "|HAS_CO_APPLICANT|count(HAS_CO_APPLICANT)|\n",
      "+----------------+-----------------------+\n",
      "|             YES|                 268613|\n",
      "|              NO|                 731387|\n",
      "+----------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = spark.sql(\"SELECT DISTINCT HAS_CO_APPLICANT, COUNT(HAS_CO_APPLICANT) FROM credit GROUP BY HAS_CO_APPLICANT\")\n",
    "t.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "|HAS_GUARANTOR|count(HAS_GUARANTOR)|\n",
      "+-------------+--------------------+\n",
      "|          YES|              397798|\n",
      "|           NO|              602202|\n",
      "+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = spark.sql(\"SELECT DISTINCT HAS_GUARANTOR, COUNT(HAS_GUARANTOR) FROM credit GROUP BY HAS_GUARANTOR\")\n",
    "t.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|IS_DEFAULT|count(IS_DEFAULT)|\n",
      "+----------+-----------------+\n",
      "|        No|           700000|\n",
      "|       Yes|           300000|\n",
      "+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = spark.sql(\"SELECT DISTINCT IS_DEFAULT, COUNT(IS_DEFAULT) FROM credit GROUP BY IS_DEFAULT\")\n",
    "t.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`No` has the higher frequency. Therefore, when using the StringIndexer, `No` will be given the index 0. `Yes` will be given the index 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------------+\n",
      "|IS_STATE_BORDER|count(IS_STATE_BORDER)|\n",
      "+---------------+----------------------+\n",
      "|            YES|                477983|\n",
      "|             NO|                522017|\n",
      "+---------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = spark.sql(\"SELECT DISTINCT IS_STATE_BORDER, COUNT(IS_STATE_BORDER) FROM credit GROUP BY IS_STATE_BORDER\")\n",
    "t.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+\n",
      "|IS_URBAN|count(IS_URBAN)|\n",
      "+--------+---------------+\n",
      "|     YES|         623097|\n",
      "|      NO|         376903|\n",
      "+--------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = spark.sql(\"SELECT DISTINCT IS_URBAN, COUNT(IS_URBAN) FROM credit GROUP BY IS_URBAN\")\n",
    "t.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------------+\n",
      "|NUMBER_CREDITS|count(NUMBER_CREDITS)|\n",
      "+--------------+---------------------+\n",
      "|             0|              1000000|\n",
      "+--------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = spark.sql(\"SELECT DISTINCT NUMBER_CREDITS, COUNT(NUMBER_CREDITS) FROM credit GROUP BY NUMBER_CREDITS\")\n",
    "t.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No need to include `NUMBER_CREDITS` feature, it is only zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+----------------------------+\n",
      "|OTHER_INSTALMENT_PLAN|count(OTHER_INSTALMENT_PLAN)|\n",
      "+---------------------+----------------------------+\n",
      "|                  YES|                      499274|\n",
      "|                   NO|                      500726|\n",
      "+---------------------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = spark.sql(\"SELECT DISTINCT OTHER_INSTALMENT_PLAN, COUNT(OTHER_INSTALMENT_PLAN) FROM credit GROUP BY OTHER_INSTALMENT_PLAN\")\n",
    "t.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+\n",
      "|OWN_CAR|count(OWN_CAR)|\n",
      "+-------+--------------+\n",
      "|    YES|        632143|\n",
      "|     NO|        367857|\n",
      "+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = spark.sql(\"SELECT DISTINCT OWN_CAR, COUNT(OWN_CAR) FROM credit GROUP BY OWN_CAR\")\n",
    "t.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------------+\n",
      "|OWN_REAL_ESTATE|count(OWN_REAL_ESTATE)|\n",
      "+---------------+----------------------+\n",
      "|            YES|                427453|\n",
      "|             NO|                572547|\n",
      "+---------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = spark.sql(\"SELECT DISTINCT OWN_REAL_ESTATE, COUNT(OWN_REAL_ESTATE) FROM credit GROUP BY OWN_REAL_ESTATE\")\n",
    "t.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "|OWN_RESIDENCE|count(OWN_RESIDENCE)|\n",
      "+-------------+--------------------+\n",
      "|          YES|              473235|\n",
      "|           NO|              526765|\n",
      "+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = spark.sql(\"SELECT DISTINCT OWN_RESIDENCE, COUNT(OWN_RESIDENCE) FROM credit GROUP BY OWN_RESIDENCE\")\n",
    "t.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+\n",
      "|RFM_SCORE|count(RFM_SCORE)|\n",
      "+---------+----------------+\n",
      "|        1|           11460|\n",
      "|        2|            3375|\n",
      "|        3|          407275|\n",
      "|        4|          577890|\n",
      "+---------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = spark.sql(\"SELECT DISTINCT RFM_SCORE, COUNT(RFM_SCORE) FROM credit GROUP BY RFM_SCORE ORDER BY RFM_SCORE\")\n",
    "t.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only four different scores, but there is intrinsic ordering. So either use as-is. Might want to consider clubbing 1&2 and using as categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+-------------------------+\n",
      "|SHIP_INTERNATIONAL|IS_DEFAULT|count(SHIP_INTERNATIONAL)|\n",
      "+------------------+----------+-------------------------+\n",
      "|                NO|        No|                   333691|\n",
      "|                NO|       Yes|                   142762|\n",
      "|               YES|        No|                   366309|\n",
      "|               YES|       Yes|                   157238|\n",
      "+------------------+----------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = spark.sql(\"SELECT DISTINCT SHIP_INTERNATIONAL, IS_DEFAULT, COUNT(SHIP_INTERNATIONAL) FROM credit \\\n",
    "GROUP BY SHIP_INTERNATIONAL, IS_DEFAULT \\\n",
    "ORDER BY SHIP_INTERNATIONAL, IS_DEFAULT\")\n",
    "t.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "International or not probably has little to do with default directly. May have something to do with transaction amount?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------------+-----------------------+-----------------------------------------------+\n",
      "|SHIP_INTERNATIONAL|count(SHIP_INTERNATIONAL)|avg(TRANSACTION_AMOUNT)|stddev_samp(CAST(TRANSACTION_AMOUNT AS DOUBLE))|\n",
      "+------------------+-------------------------+-----------------------+-----------------------------------------------+\n",
      "|                NO|                   476453|     26789.872356769712|                              5102.042903503068|\n",
      "|               YES|                   523547|     27153.019362158506|                             4098.4969321578565|\n",
      "+------------------+-------------------------+-----------------------+-----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = spark.sql(\"SELECT DISTINCT SHIP_INTERNATIONAL, COUNT(SHIP_INTERNATIONAL), MEAN(TRANSACTION_AMOUNT), STD(TRANSACTION_AMOUNT) FROM credit \\\n",
    "GROUP BY SHIP_INTERNATIONAL \\\n",
    "ORDER BY SHIP_INTERNATIONAL\")\n",
    "t.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "International shipping or not has little to do with transaction amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|STATE|counts|\n",
      "+-----+------+\n",
      "|   NJ|  4582|\n",
      "|   NY| 13435|\n",
      "|   PA|416961|\n",
      "|   CT|565022|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = spark.sql(\"SELECT DISTINCT STATE, COUNT(STATE) as counts FROM credit GROUP BY STATE ORDER BY counts\")\n",
    "t.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, only 4 states. NJ and NY have 'few' samples percentage-wise. Make as categorical. One would guess it shouldn't depend on the state. Also the 4 states border each other. May want to look at other features such as population/GDP just to see what it's like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-----------------------------------------------+\n",
      "|avg(TRANSACTION_AMOUNT)|stddev_samp(CAST(TRANSACTION_AMOUNT AS DOUBLE))|\n",
      "+-----------------------+-----------------------------------------------+\n",
      "|      28820.70300857143|                              3243.905091234831|\n",
      "|            22685.01592|                              4456.413935434757|\n",
      "+-----------------------+-----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = spark.sql(\"SELECT MEAN(TRANSACTION_AMOUNT), STD(TRANSACTION_AMOUNT) FROM credit GROUP BY IS_DEFAULT\")\n",
    "t.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|TRANSACTION_CATEGORY|counts|\n",
      "+--------------------+------+\n",
      "|               OTHER|   686|\n",
      "|             NEW CAR|   762|\n",
      "|          RETRAINING| 27271|\n",
      "|            USED CAR| 41880|\n",
      "|           EDUCATION|186619|\n",
      "|         ELECTRONICS|316553|\n",
      "|           FURNITURE|426229|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = spark.sql(\"SELECT TRANSACTION_CATEGORY, COUNT(TRANSACTION_CATEGORY) as counts FROM credit\\\n",
    "              GROUP BY TRANSACTION_CATEGORY \\\n",
    "              ORDER BY counts\")\n",
    "t.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+---------------------------+\n",
      "|TRANSACTION_CATEGORY|IS_DEFAULT|count(TRANSACTION_CATEGORY)|\n",
      "+--------------------+----------+---------------------------+\n",
      "|           EDUCATION|        No|                      45615|\n",
      "|           EDUCATION|       Yes|                     141004|\n",
      "|         ELECTRONICS|        No|                     301451|\n",
      "|         ELECTRONICS|       Yes|                      15102|\n",
      "|           FURNITURE|        No|                     309836|\n",
      "|           FURNITURE|       Yes|                     116393|\n",
      "|             NEW CAR|        No|                        758|\n",
      "|             NEW CAR|       Yes|                          4|\n",
      "|               OTHER|        No|                          4|\n",
      "|               OTHER|       Yes|                        682|\n",
      "|          RETRAINING|        No|                        914|\n",
      "|          RETRAINING|       Yes|                      26357|\n",
      "|            USED CAR|        No|                      41422|\n",
      "|            USED CAR|       Yes|                        458|\n",
      "+--------------------+----------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = spark.sql(\"SELECT DISTINCT TRANSACTION_CATEGORY, IS_DEFAULT, COUNT(TRANSACTION_CATEGORY) FROM credit \\\n",
    "GROUP BY TRANSACTION_CATEGORY, IS_DEFAULT ORDER BY TRANSACTION_CATEGORY, IS_DEFAULT\")\n",
    "t.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Education, retraining and 'other' category more likely to default than not. Odds of defaulting:not defaulting for Electronics is 1:20, Furniture is 1:3, used car is 1:100. This is probably one of the first features we should model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(DenseVector([3.5516, 26979.9969, 109.6766]), DenseVector([0.5682, 4607.5721, 18.856]))\n"
     ]
    }
   ],
   "source": [
    "va1 = VectorAssembler(inputCols=['RFM_SCORE', 'TRANSACTION_AMOUNT', 'EMI_TENURE'], outputCol='rfm_amt_emi')\n",
    "df2 = va1.transform(df)\n",
    "ss = StandardScaler(withMean=True, inputCol='rfm_amt_emi', outputCol='rfm_amt_emi_scaled')\n",
    "ssmodel = ss.fit(df2)\n",
    "df2 = ssmodel.transform(df2)\n",
    "print(ssmodel.mean, ssmodel.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(DenseVector([0.0, -0.0, 0.0]), DenseVector([1.0, 1.0, 1.0]))\n"
     ]
    }
   ],
   "source": [
    "ss2 = StandardScaler(inputCol=\"rfm_amt_emi_scaled\", outputCol=\"scaled_twice\")\n",
    "ssmodel2 = ss2.fit(df2)\n",
    "print(ssmodel2.mean, ssmodel2.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ACCOUNT_AGE=u'up to 1 YR', ACCOUNT_TYPE=u'above 1000 K USD', CREDIT_HISTORY=u'EXISTING CREDITS PAID BACK', EMI_TENURE=105, HAS_CO_APPLICANT=u'NO', HAS_GUARANTOR=u'YES', IS_DEFAULT=u'No', IS_STATE_BORDER=u'YES', IS_URBAN=u'YES', NUMBER_CREDITS=0, OTHER_INSTALMENT_PLAN=u'NO', OWN_CAR=u'YES', OWN_REAL_ESTATE=u'YES', OWN_RESIDENCE=u'YES', RFM_SCORE=4, SHIP_INTERNATIONAL=u'YES', STATE=u'CT', TRANSACTION_AMOUNT=25788, TRANSACTION_CATEGORY=u'EDUCATION', rfm_amt_emi=DenseVector([4.0, 25788.0, 105.0]), rfm_amt_emi_scaled=DenseVector([0.7892, -0.2587, -0.248]), label=0.0, account_age_ind=0.0, account_type_ind=0.0, credit_history_ind=0.0, has_co_applicant_ind=0.0, has_guarantor_ind=1.0, is_state_border_ind=1.0, is_urban_ind=0.0, other_instalment_plan_ind=0.0, own_car_ind=0.0, own_real_estate_ind=1.0, own_residence_ind=1.0, ship_international_ind=0.0, state_ind=0.0, transaction_category_ind=2.0, account_age_vec=SparseVector(4, {0: 1.0}), account_type_vec=SparseVector(4, {0: 1.0}), credit_history_vec=SparseVector(4, {0: 1.0}), transaction_vec=SparseVector(6, {2: 1.0}), state_vec=SparseVector(3, {0: 1.0}), features=SparseVector(33, {0: 1.0, 4: 1.0, 8: 1.0, 13: 1.0, 14: 1.0, 18: 1.0, 19: 1.0, 21: 1.0, 26: 1.0, 30: 0.7892, 31: -0.2587, 32: -0.248})),\n",
       " Row(ACCOUNT_AGE=u'up to 1 YR', ACCOUNT_TYPE=u'UNKNOWN/NONE', CREDIT_HISTORY=u'EXISTING CREDITS PAID BACK', EMI_TENURE=105, HAS_CO_APPLICANT=u'NO', HAS_GUARANTOR=u'NO', IS_DEFAULT=u'No', IS_STATE_BORDER=u'NO', IS_URBAN=u'YES', NUMBER_CREDITS=0, OTHER_INSTALMENT_PLAN=u'YES', OWN_CAR=u'YES', OWN_REAL_ESTATE=u'NO', OWN_RESIDENCE=u'NO', RFM_SCORE=3, SHIP_INTERNATIONAL=u'YES', STATE=u'CT', TRANSACTION_AMOUNT=25788, TRANSACTION_CATEGORY=u'FURNITURE', rfm_amt_emi=DenseVector([3.0, 25788.0, 105.0]), rfm_amt_emi_scaled=DenseVector([-0.9708, -0.2587, -0.248]), label=0.0, account_age_ind=0.0, account_type_ind=1.0, credit_history_ind=0.0, has_co_applicant_ind=0.0, has_guarantor_ind=0.0, is_state_border_ind=0.0, is_urban_ind=0.0, other_instalment_plan_ind=1.0, own_car_ind=0.0, own_real_estate_ind=0.0, own_residence_ind=0.0, ship_international_ind=0.0, state_ind=0.0, transaction_category_ind=0.0, account_age_vec=SparseVector(4, {0: 1.0}), account_type_vec=SparseVector(4, {1: 1.0}), credit_history_vec=SparseVector(4, {0: 1.0}), transaction_vec=SparseVector(6, {0: 1.0}), state_vec=SparseVector(3, {0: 1.0}), features=SparseVector(33, {0: 1.0, 5: 1.0, 8: 1.0, 16: 1.0, 21: 1.0, 24: 1.0, 30: -0.9708, 31: -0.2587, 32: -0.248})),\n",
       " Row(ACCOUNT_AGE=u'up to 1 YR', ACCOUNT_TYPE=u'UNKNOWN/NONE', CREDIT_HISTORY=u'EXISTING CREDITS PAID BACK', EMI_TENURE=112, HAS_CO_APPLICANT=u'NO', HAS_GUARANTOR=u'NO', IS_DEFAULT=u'No', IS_STATE_BORDER=u'YES', IS_URBAN=u'YES', NUMBER_CREDITS=0, OTHER_INSTALMENT_PLAN=u'YES', OWN_CAR=u'YES', OWN_REAL_ESTATE=u'NO', OWN_RESIDENCE=u'NO', RFM_SCORE=3, SHIP_INTERNATIONAL=u'NO', STATE=u'CT', TRANSACTION_AMOUNT=27630, TRANSACTION_CATEGORY=u'FURNITURE', rfm_amt_emi=DenseVector([3.0, 27630.0, 112.0]), rfm_amt_emi_scaled=DenseVector([-0.9708, 0.1411, 0.1232]), label=0.0, account_age_ind=0.0, account_type_ind=1.0, credit_history_ind=0.0, has_co_applicant_ind=0.0, has_guarantor_ind=0.0, is_state_border_ind=1.0, is_urban_ind=0.0, other_instalment_plan_ind=1.0, own_car_ind=0.0, own_real_estate_ind=0.0, own_residence_ind=0.0, ship_international_ind=1.0, state_ind=0.0, transaction_category_ind=0.0, account_age_vec=SparseVector(4, {0: 1.0}), account_type_vec=SparseVector(4, {1: 1.0}), credit_history_vec=SparseVector(4, {0: 1.0}), transaction_vec=SparseVector(6, {0: 1.0}), state_vec=SparseVector(3, {0: 1.0}), features=SparseVector(33, {0: 1.0, 5: 1.0, 8: 1.0, 14: 1.0, 16: 1.0, 20: 1.0, 21: 1.0, 24: 1.0, 30: -0.9708, 31: 0.1411, 32: 0.1232}))]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexer = StringIndexer(inputCol=\"IS_DEFAULT\", outputCol=\"label\")\n",
    "ndf = indexer.fit(df2).transform(df2)\n",
    "names = [\"ACCOUNT_AGE\", \"ACCOUNT_TYPE\", \"CREDIT_HISTORY\", \n",
    "         \"HAS_CO_APPLICANT\", \"HAS_GUARANTOR\", \"IS_STATE_BORDER\", \n",
    "         \"IS_URBAN\", \"OTHER_INSTALMENT_PLAN\",\"OWN_CAR\", \n",
    "         \"OWN_REAL_ESTATE\", \"OWN_RESIDENCE\", \"SHIP_INTERNATIONAL\", \n",
    "         \"STATE\", \"TRANSACTION_CATEGORY\"]\n",
    "feat_names = []\n",
    "feat_base = []\n",
    "for ind, name in enumerate(names):\n",
    "    inpcol = name\n",
    "    oupcol = name.lower() + \"_ind\"\n",
    "    indexer = StringIndexer(inputCol = inpcol, outputCol = oupcol)\n",
    "    model = indexer.fit(ndf)\n",
    "    ndf = model.transform(ndf)\n",
    "    this_labels = model.labels[:-1]\n",
    "    [feat_names.append(inpcol + \"(\" + a_label + \")\") for a_label in this_labels]\n",
    "    feat_base.append(inpcol + \"(\" + model.labels[-1] + \")\")\n",
    "encoder = OneHotEncoder(inputCol=\"account_age_ind\", outputCol=\"account_age_vec\")\n",
    "ndf = encoder.transform(ndf)\n",
    "encoder = OneHotEncoder(inputCol=\"account_type_ind\", outputCol=\"account_type_vec\")\n",
    "ndf = encoder.transform(ndf)\n",
    "encoder = OneHotEncoder(inputCol=\"credit_history_ind\", outputCol=\"credit_history_vec\")\n",
    "ndf = encoder.transform(ndf)\n",
    "encoder = OneHotEncoder(inputCol=\"transaction_category_ind\", outputCol=\"transaction_vec\")\n",
    "ndf = encoder.transform(ndf)\n",
    "encoder = OneHotEncoder(inputCol=\"state_ind\", outputCol=\"state_vec\")\n",
    "ndf = encoder.transform(ndf)\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"account_age_vec\", \"account_type_vec\", \"credit_history_vec\",\n",
    "               \"has_co_applicant_ind\", \"has_guarantor_ind\", \"is_state_border_ind\", \n",
    "               \"is_urban_ind\", \"other_instalment_plan_ind\", \"own_car_ind\", \n",
    "               \"own_real_estate_ind\", \"own_residence_ind\", \"ship_international_ind\",\n",
    "               \"state_vec\", \"transaction_vec\",\n",
    "               \"rfm_amt_emi_scaled\"],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "ndf = assembler.transform(ndf)\n",
    "ndf.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'ACCOUNT_AGE(1 to 4 YRS)', u'ACCOUNT_TYPE(500 to 1000 K USD)', u'CREDIT_HISTORY(CRITICAL ACCOUNT)', u'HAS_CO_APPLICANT(YES)', u'HAS_GUARANTOR(YES)', u'IS_STATE_BORDER(YES)', u'IS_URBAN(NO)', u'OTHER_INSTALMENT_PLAN(YES)', u'OWN_CAR(NO)', u'OWN_REAL_ESTATE(YES)', u'OWN_RESIDENCE(YES)', u'SHIP_INTERNATIONAL(NO)', u'STATE(NJ)', u'TRANSACTION_CATEGORY(OTHER)', 'rfm = 3.55', 'transaction amount = 26980.0', 'emi tenure = 109.68']\n"
     ]
    }
   ],
   "source": [
    "feat_names_full = feat_names + [\"scaled rfm\", \"scaled transaction amount\", \"scaled emi tenure\"]\n",
    "feat_base_full = feat_base + [\"rfm = \" + str(round(ssmodel.mean[0], 2)), \n",
    "                              \"transaction amount = \" + str(round(ssmodel.mean[1], 2)),\n",
    "                              \"emi tenure = \" + str(round(ssmodel.mean[2], 2))]\n",
    "print(feat_base_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"feat_names.npz\", feat_base_full=feat_base_full, feat_names_full=feat_names_full,\n",
    "        mean=ssmodel.mean.toArray(), std=ssmodel.std.toArray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf.select(\"label\", \"features\").write.mode('overwrite').save(\"credit_feats.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 2.1",
   "language": "python",
   "name": "python2-spark21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
