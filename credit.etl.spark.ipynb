{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20191029165525-0003\nKERNEL_ID = e02be0e4-9044-411f-85a3-02b5274d6424\n"},{"data":{"text/plain":"[Row(ACCOUNT_AGE='up to 1 YR', ACCOUNT_TYPE='above 1000 K USD', CREDIT_HISTORY='EXISTING CREDITS PAID BACK', EMI_TENURE=105, HAS_CO_APPLICANT='NO', HAS_GUARANTOR='YES', IS_DEFAULT='No', IS_STATE_BORDER='YES', IS_URBAN='YES', NUMBER_CREDITS=0, OTHER_INSTALMENT_PLAN='NO', OWN_CAR='YES', OWN_REAL_ESTATE='YES', OWN_RESIDENCE='YES', RFM_SCORE=4, SHIP_INTERNATIONAL='YES', STATE='CT', TRANSACTION_AMOUNT=25788, TRANSACTION_CATEGORY='EDUCATION'),\n Row(ACCOUNT_AGE='up to 1 YR', ACCOUNT_TYPE='UNKNOWN/NONE', CREDIT_HISTORY='EXISTING CREDITS PAID BACK', EMI_TENURE=105, HAS_CO_APPLICANT='NO', HAS_GUARANTOR='NO', IS_DEFAULT='No', IS_STATE_BORDER='NO', IS_URBAN='YES', NUMBER_CREDITS=0, OTHER_INSTALMENT_PLAN='YES', OWN_CAR='YES', OWN_REAL_ESTATE='NO', OWN_RESIDENCE='NO', RFM_SCORE=3, SHIP_INTERNATIONAL='YES', STATE='CT', TRANSACTION_AMOUNT=25788, TRANSACTION_CATEGORY='FURNITURE'),\n Row(ACCOUNT_AGE='up to 1 YR', ACCOUNT_TYPE='UNKNOWN/NONE', CREDIT_HISTORY='EXISTING CREDITS PAID BACK', EMI_TENURE=112, HAS_CO_APPLICANT='NO', HAS_GUARANTOR='NO', IS_DEFAULT='No', IS_STATE_BORDER='YES', IS_URBAN='YES', NUMBER_CREDITS=0, OTHER_INSTALMENT_PLAN='YES', OWN_CAR='YES', OWN_REAL_ESTATE='NO', OWN_RESIDENCE='NO', RFM_SCORE=3, SHIP_INTERNATIONAL='NO', STATE='CT', TRANSACTION_AMOUNT=27630, TRANSACTION_CATEGORY='FURNITURE'),\n Row(ACCOUNT_AGE='above 7 YRS', ACCOUNT_TYPE='above 1000 K USD', CREDIT_HISTORY='ALL CREDITS PAID BACK', EMI_TENURE=140, HAS_CO_APPLICANT='YES', HAS_GUARANTOR='NO', IS_DEFAULT='No', IS_STATE_BORDER='NO', IS_URBAN='YES', NUMBER_CREDITS=0, OTHER_INSTALMENT_PLAN='NO', OWN_CAR='NO', OWN_REAL_ESTATE='NO', OWN_RESIDENCE='NO', RFM_SCORE=3, SHIP_INTERNATIONAL='NO', STATE='PA', TRANSACTION_AMOUNT=31314, TRANSACTION_CATEGORY='USED CAR'),\n Row(ACCOUNT_AGE='up to 1 YR', ACCOUNT_TYPE='above 1000 K USD', CREDIT_HISTORY='EXISTING CREDITS PAID BACK', EMI_TENURE=98, HAS_CO_APPLICANT='YES', HAS_GUARANTOR='YES', IS_DEFAULT='Yes', IS_STATE_BORDER='NO', IS_URBAN='YES', NUMBER_CREDITS=0, OTHER_INSTALMENT_PLAN='YES', OWN_CAR='YES', OWN_REAL_ESTATE='NO', OWN_RESIDENCE='NO', RFM_SCORE=4, SHIP_INTERNATIONAL='NO', STATE='PA', TRANSACTION_AMOUNT=25788, TRANSACTION_CATEGORY='FURNITURE')]"},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import ibmos2spark \n","# credentials = { 'endpoint': 'https://s3-api.us-geo.objectstorage.service.networklayer.com', 'service_id': '___', 'iam_service_endpoint': 'https://iam.ng.bluemix.net/oidc/token', 'api_key': '___' } \n","configuration_name = 'os_ddb7527012204f00b53a3cb6cde76be3_configs' \n","cos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos') \n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.getOrCreate() \n","df = spark.read.format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n",".option('header', 'true')\\\n",".option('inferSchema', 'true')\\\n",".load(cos.url('credit_customer_history.csv', 'default-donotdelete-pr-bnveggkfxwmoac')) \n","df.take(5) "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# from pyspark.sql import SparkSession\n","# spark = SparkSession.builder.getOrCreate()\n","# df = spark.read\\\n","#   .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n","#   .option('header', 'true')\\\n","#   .option('inferSchema', 'true')\\\n","#   .load(cos.url('credit_customer_history.csv', 'default-donotdelete-pr-bnveggkfxwmoac'))\n","# df.take(5)\n"]},{"cell_type":"markdown","metadata":{},"source":["## What are the column names?"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"root\n |-- ACCOUNT_AGE: string (nullable = true)\n |-- ACCOUNT_TYPE: string (nullable = true)\n |-- CREDIT_HISTORY: string (nullable = true)\n |-- EMI_TENURE: integer (nullable = true)\n |-- HAS_CO_APPLICANT: string (nullable = true)\n |-- HAS_GUARANTOR: string (nullable = true)\n |-- IS_DEFAULT: string (nullable = true)\n |-- IS_STATE_BORDER: string (nullable = true)\n |-- IS_URBAN: string (nullable = true)\n |-- NUMBER_CREDITS: integer (nullable = true)\n |-- OTHER_INSTALMENT_PLAN: string (nullable = true)\n |-- OWN_CAR: string (nullable = true)\n |-- OWN_REAL_ESTATE: string (nullable = true)\n |-- OWN_RESIDENCE: string (nullable = true)\n |-- RFM_SCORE: integer (nullable = true)\n |-- SHIP_INTERNATIONAL: string (nullable = true)\n |-- STATE: string (nullable = true)\n |-- TRANSACTION_AMOUNT: integer (nullable = true)\n |-- TRANSACTION_CATEGORY: string (nullable = true)\n\n"}],"source":["df.printSchema()"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n","import numpy as np"]},{"cell_type":"markdown","metadata":{},"source":["The dataset is available here: https://developer.ibm.com/blogs/snap-ml-use-cases-blog/ , credit default prediction. Quoting the text:\n","> The task in this use case is to predict whether a person who has credit will default (not be able to repay his credit). The data scientist is provided with a data set of 10 million transactions, each of which is characterized by 18 features (including account age, account type, credit history, owns car, transaction amount, and transaction category). Also provided are the labels of these transactions, default or not. The task is to build a model to predict whether transactions will default in an unseen data set, that is, a data set that has not been used to train the model and does not have labels."]},{"cell_type":"markdown","metadata":{},"source":["## Exploration\n","\n","First create a temporary view to run SQL queries."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["df.createOrReplaceTempView(\"credit\")"]},{"cell_type":"markdown","metadata":{},"source":["### Feature by feature\n","\n","Look at each feature, see what values it can be and counts.\n","Also look at classification split of interesting features to see if dataset is balanced."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"+-----------+----------+\n|ACCOUNT_AGE|age_counts|\n+-----------+----------+\n| 1 to 4 YRS|      1093|\n|        TBD|      1581|\n| 4 to 7 YRS|    141090|\n|above 7 YRS|    165109|\n| up to 1 YR|    691127|\n+-----------+----------+\n\n"}],"source":["t = spark.sql(\"SELECT DISTINCT ACCOUNT_AGE, COUNT(ACCOUNT_AGE) AS age_counts FROM credit \\\n","GROUP BY ACCOUNT_AGE \\\n","ORDER BY age_counts\")\n","t.show()"]},{"cell_type":"markdown","metadata":{},"source":["- Many accounts are new.\n","- Strangely very few accounts in 1-4 year range (maybe students?)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"+-----------------+-----------+\n|     ACCOUNT_TYPE|type_counts|\n+-----------------+-----------+\n|500 to 1000 K USD|       1472|\n|  up to 100 K USD|       2578|\n| 100 to 500 K USD|     141594|\n|     UNKNOWN/NONE|     188476|\n| above 1000 K USD|     665880|\n+-----------------+-----------+\n\n"}],"source":["t = spark.sql(\"SELECT DISTINCT ACCOUNT_TYPE, COUNT(ACCOUNT_TYPE) AS type_counts FROM credit \\\n","GROUP BY ACCOUNT_TYPE \\\n","ORDER BY type_counts\")\n","t.show()"]},{"cell_type":"markdown","metadata":{},"source":["- Wow, many transactions from millionaires. Maybe student hypothesis is wrong..."]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"+-----------------+----------+-------------------+\n|     ACCOUNT_TYPE|IS_DEFAULT|count(ACCOUNT_TYPE)|\n+-----------------+----------+-------------------+\n| 100 to 500 K USD|        No|              94719|\n| 100 to 500 K USD|       Yes|              46875|\n|500 to 1000 K USD|        No|                667|\n|500 to 1000 K USD|       Yes|                805|\n|     UNKNOWN/NONE|        No|             127634|\n|     UNKNOWN/NONE|       Yes|              60842|\n| above 1000 K USD|        No|             475662|\n| above 1000 K USD|       Yes|             190218|\n|  up to 100 K USD|        No|               1318|\n|  up to 100 K USD|       Yes|               1260|\n+-----------------+----------+-------------------+\n\n"}],"source":["t = spark.sql(\"SELECT DISTINCT ACCOUNT_TYPE, IS_DEFAULT, COUNT(ACCOUNT_TYPE) FROM credit \\\n","GROUP BY ACCOUNT_TYPE, IS_DEFAULT \\\n","ORDER BY ACCOUNT_TYPE, IS_DEFAULT\")\n","t.show()"]},{"cell_type":"markdown","metadata":{},"source":["- Up to 100 K and 500-1000k have 1:1 odds of defaulting:not defaulting, compared to 1:2 of 100-500k and >1000k. "]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"+--------------------+---------------------+\n|      CREDIT_HISTORY|count(CREDIT_HISTORY)|\n+--------------------+---------------------+\n|          NONE TAKEN|                 7635|\n|ALL CREDITS PAID ...|               456194|\n|       DELAY IN PAST|                61922|\n|EXISTING CREDITS ...|               473511|\n|    CRITICAL ACCOUNT|                  738|\n+--------------------+---------------------+\n\n"}],"source":["t = spark.sql(\"SELECT DISTINCT CREDIT_HISTORY, COUNT(CREDIT_HISTORY) FROM credit GROUP BY CREDIT_HISTORY\")\n","t.show()"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"+----------+-----------------+---------------------------------------+\n|IS_DEFAULT|  avg(EMI_TENURE)|stddev_samp(CAST(EMI_TENURE AS DOUBLE))|\n+----------+-----------------+---------------------------------------+\n|        No|        116.77337|                     14.091804445397342|\n|       Yes|93.11754666666667|                     18.168372237132022|\n+----------+-----------------+---------------------------------------+\n\n"}],"source":["t = spark.sql(\"SELECT IS_DEFAULT, MEAN(EMI_TENURE), STD(EMI_TENURE) FROM credit GROUP BY IS_DEFAULT\")\n","t.show()"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"+---------------+---------------------------------------+\n|avg(EMI_TENURE)|stddev_samp(CAST(EMI_TENURE AS DOUBLE))|\n+---------------+---------------------------------------+\n|     109.676623|                      18.85596971421701|\n+---------------+---------------------------------------+\n\n"}],"source":["t = spark.sql(\"SELECT MEAN(EMI_TENURE), STD(EMI_TENURE) FROM credit\")\n","t.show()"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"+----------------+-----------------------+\n|HAS_CO_APPLICANT|count(HAS_CO_APPLICANT)|\n+----------------+-----------------------+\n|             YES|                 268613|\n|              NO|                 731387|\n+----------------+-----------------------+\n\n"}],"source":["t = spark.sql(\"SELECT DISTINCT HAS_CO_APPLICANT, COUNT(HAS_CO_APPLICANT) FROM credit GROUP BY HAS_CO_APPLICANT\")\n","t.show()"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"+-------------+--------------------+\n|HAS_GUARANTOR|count(HAS_GUARANTOR)|\n+-------------+--------------------+\n|          YES|              397798|\n|           NO|              602202|\n+-------------+--------------------+\n\n"}],"source":["t = spark.sql(\"SELECT DISTINCT HAS_GUARANTOR, COUNT(HAS_GUARANTOR) FROM credit GROUP BY HAS_GUARANTOR\")\n","t.show()"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"+----------+-----------------+\n|IS_DEFAULT|count(IS_DEFAULT)|\n+----------+-----------------+\n|        No|           700000|\n|       Yes|           300000|\n+----------+-----------------+\n\n"}],"source":["t = spark.sql(\"SELECT DISTINCT IS_DEFAULT, COUNT(IS_DEFAULT) FROM credit GROUP BY IS_DEFAULT\")\n","t.show()"]},{"cell_type":"markdown","metadata":{},"source":["`No` has the higher frequency. Therefore, when using the StringIndexer, `No` will be given the index 0. `Yes` will be given the index 1."]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"+---------------+----------------------+\n|IS_STATE_BORDER|count(IS_STATE_BORDER)|\n+---------------+----------------------+\n|            YES|                477983|\n|             NO|                522017|\n+---------------+----------------------+\n\n"}],"source":["t = spark.sql(\"SELECT DISTINCT IS_STATE_BORDER, COUNT(IS_STATE_BORDER) FROM credit GROUP BY IS_STATE_BORDER\")\n","t.show()"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"+--------+---------------+\n|IS_URBAN|count(IS_URBAN)|\n+--------+---------------+\n|     YES|         623097|\n|      NO|         376903|\n+--------+---------------+\n\n"}],"source":["t = spark.sql(\"SELECT DISTINCT IS_URBAN, COUNT(IS_URBAN) FROM credit GROUP BY IS_URBAN\")\n","t.show()"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"+--------------+---------------------+\n|NUMBER_CREDITS|count(NUMBER_CREDITS)|\n+--------------+---------------------+\n|             0|              1000000|\n+--------------+---------------------+\n\n"}],"source":["t = spark.sql(\"SELECT DISTINCT NUMBER_CREDITS, COUNT(NUMBER_CREDITS) FROM credit GROUP BY NUMBER_CREDITS\")\n","t.show()"]},{"cell_type":"markdown","metadata":{},"source":["No need to include `NUMBER_CREDITS` feature, it is only zeros."]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"+---------------------+----------------------------+\n|OTHER_INSTALMENT_PLAN|count(OTHER_INSTALMENT_PLAN)|\n+---------------------+----------------------------+\n|                  YES|                      499274|\n|                   NO|                      500726|\n+---------------------+----------------------------+\n\n"}],"source":["t = spark.sql(\"SELECT DISTINCT OTHER_INSTALMENT_PLAN, COUNT(OTHER_INSTALMENT_PLAN) FROM credit GROUP BY OTHER_INSTALMENT_PLAN\")\n","t.show()"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"+-------+--------------+\n|OWN_CAR|count(OWN_CAR)|\n+-------+--------------+\n|    YES|        632143|\n|     NO|        367857|\n+-------+--------------+\n\n"}],"source":["t = spark.sql(\"SELECT DISTINCT OWN_CAR, COUNT(OWN_CAR) FROM credit GROUP BY OWN_CAR\")\n","t.show()"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"+---------------+----------------------+\n|OWN_REAL_ESTATE|count(OWN_REAL_ESTATE)|\n+---------------+----------------------+\n|            YES|                427453|\n|             NO|                572547|\n+---------------+----------------------+\n\n"}],"source":["t = spark.sql(\"SELECT DISTINCT OWN_REAL_ESTATE, COUNT(OWN_REAL_ESTATE) FROM credit GROUP BY OWN_REAL_ESTATE\")\n","t.show()"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"+-------------+--------------------+\n|OWN_RESIDENCE|count(OWN_RESIDENCE)|\n+-------------+--------------------+\n|          YES|              473235|\n|           NO|              526765|\n+-------------+--------------------+\n\n"}],"source":["t = spark.sql(\"SELECT DISTINCT OWN_RESIDENCE, COUNT(OWN_RESIDENCE) FROM credit GROUP BY OWN_RESIDENCE\")\n","t.show()"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"+---------+----------------+\n|RFM_SCORE|count(RFM_SCORE)|\n+---------+----------------+\n|        1|           11460|\n|        2|            3375|\n|        3|          407275|\n|        4|          577890|\n+---------+----------------+\n\n"}],"source":["t = spark.sql(\"SELECT DISTINCT RFM_SCORE, COUNT(RFM_SCORE) FROM credit GROUP BY RFM_SCORE ORDER BY RFM_SCORE\")\n","t.show()"]},{"cell_type":"markdown","metadata":{},"source":["Only four different scores, but there is intrinsic ordering. So either use as-is or consider clubbing 1&2 and using as categorical."]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"+------------------+----------+-------------------------+\n|SHIP_INTERNATIONAL|IS_DEFAULT|count(SHIP_INTERNATIONAL)|\n+------------------+----------+-------------------------+\n|                NO|        No|                   333691|\n|                NO|       Yes|                   142762|\n|               YES|        No|                   366309|\n|               YES|       Yes|                   157238|\n+------------------+----------+-------------------------+\n\n"}],"source":["t = spark.sql(\"SELECT DISTINCT SHIP_INTERNATIONAL, IS_DEFAULT, COUNT(SHIP_INTERNATIONAL) FROM credit \\\n","GROUP BY SHIP_INTERNATIONAL, IS_DEFAULT \\\n","ORDER BY SHIP_INTERNATIONAL, IS_DEFAULT\")\n","t.show()"]},{"cell_type":"markdown","metadata":{},"source":["International or not probably has little to do with default directly. May have something to do with transaction amount?"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"+------------------+-------------------------+-----------------------+-----------------------------------------------+\n|SHIP_INTERNATIONAL|count(SHIP_INTERNATIONAL)|avg(TRANSACTION_AMOUNT)|stddev_samp(CAST(TRANSACTION_AMOUNT AS DOUBLE))|\n+------------------+-------------------------+-----------------------+-----------------------------------------------+\n|                NO|                   476453|     26789.872356769712|                              5102.042903503083|\n|               YES|                   523547|     27153.019362158506|                              4098.496932157878|\n+------------------+-------------------------+-----------------------+-----------------------------------------------+\n\n"}],"source":["t = spark.sql(\"SELECT DISTINCT SHIP_INTERNATIONAL, COUNT(SHIP_INTERNATIONAL), MEAN(TRANSACTION_AMOUNT), STD(TRANSACTION_AMOUNT) FROM credit \\\n","GROUP BY SHIP_INTERNATIONAL \\\n","ORDER BY SHIP_INTERNATIONAL\")\n","t.show()"]},{"cell_type":"markdown","metadata":{},"source":["International shipping or not has little to do with transaction amount."]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"+-----+------+\n|STATE|counts|\n+-----+------+\n|   NJ|  4582|\n|   NY| 13435|\n|   PA|416961|\n|   CT|565022|\n+-----+------+\n\n"}],"source":["t = spark.sql(\"SELECT DISTINCT STATE, COUNT(STATE) as counts FROM credit GROUP BY STATE ORDER BY counts\")\n","t.show()"]},{"cell_type":"markdown","metadata":{},"source":["#### Interesting\n","- Only 4 states.\n","- NJ and NY have 'few' samples percentage-wise. Make as categorical.\n","- Also the 4 states border each other. May want to look at other features such as population/GDP just to see what it's like."]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"+-----------------------+-----------------------------------------------+\n|avg(TRANSACTION_AMOUNT)|stddev_samp(CAST(TRANSACTION_AMOUNT AS DOUBLE))|\n+-----------------------+-----------------------------------------------+\n|      28820.70300857143|                             3243.9050912348657|\n|            22685.01592|                              4456.413935434743|\n+-----------------------+-----------------------------------------------+\n\n"}],"source":["t = spark.sql(\"SELECT MEAN(TRANSACTION_AMOUNT), STD(TRANSACTION_AMOUNT) FROM credit GROUP BY IS_DEFAULT\")\n","t.show()"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"+--------------------+------+\n|TRANSACTION_CATEGORY|counts|\n+--------------------+------+\n|               OTHER|   686|\n|             NEW CAR|   762|\n|          RETRAINING| 27271|\n|            USED CAR| 41880|\n|           EDUCATION|186619|\n|         ELECTRONICS|316553|\n|           FURNITURE|426229|\n+--------------------+------+\n\n"}],"source":["t = spark.sql(\"SELECT TRANSACTION_CATEGORY, COUNT(TRANSACTION_CATEGORY) as counts FROM credit\\\n","              GROUP BY TRANSACTION_CATEGORY \\\n","              ORDER BY counts\")\n","t.show()"]},{"cell_type":"markdown","metadata":{},"source":["#### Interesting\n","\n","- Most of the transactions are for Furniture, which is quite unexpected. It may be a biased dataset, but intersting nevertheless.\n","- And cars (new/used) form a 10th of the furniture transactions."]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"+--------------------+----------+---------------------------+\n|TRANSACTION_CATEGORY|IS_DEFAULT|count(TRANSACTION_CATEGORY)|\n+--------------------+----------+---------------------------+\n|           EDUCATION|        No|                      45615|\n|           EDUCATION|       Yes|                     141004|\n|         ELECTRONICS|        No|                     301451|\n|         ELECTRONICS|       Yes|                      15102|\n|           FURNITURE|        No|                     309836|\n|           FURNITURE|       Yes|                     116393|\n|             NEW CAR|        No|                        758|\n|             NEW CAR|       Yes|                          4|\n|               OTHER|        No|                          4|\n|               OTHER|       Yes|                        682|\n|          RETRAINING|        No|                        914|\n|          RETRAINING|       Yes|                      26357|\n|            USED CAR|        No|                      41422|\n|            USED CAR|       Yes|                        458|\n+--------------------+----------+---------------------------+\n\n"}],"source":["t = spark.sql(\"SELECT DISTINCT TRANSACTION_CATEGORY, IS_DEFAULT, COUNT(TRANSACTION_CATEGORY) FROM credit \\\n","GROUP BY TRANSACTION_CATEGORY, IS_DEFAULT ORDER BY TRANSACTION_CATEGORY, IS_DEFAULT\")\n","t.show()"]},{"cell_type":"markdown","metadata":{},"source":["#### Interesting\n","- Education, retraining and 'other' category more likely to default than not. \n","- Odds of defaulting:not defaulting for Electronics is 1:20, Furniture is 1:3, used car is 1:100."]},{"cell_type":"markdown","metadata":{},"source":["# Loading\n","\n","- Transform the data into categorical or centered numeric features for classification.\n","\n","## Tools\n","- `VectorAssembler` - makes a column of numerical vectors from input columns\n","- `StandardScaler` - make mean = 0, std = 1\n","- `StringIndexer` - convert strings to categorical variables with lower index representing more frequent sample in dataset\n","- `OneHotEncoder` - converts categorical variables to one-hot encoding\n","\n","First assemble the numerical values into a vector and scale them."]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"[3.5515949999999714,26979.996881999785,109.67662299999911] [0.5681973942418947,4607.57206512009,18.855969714217142]\n"}],"source":["va1 = VectorAssembler(inputCols=['RFM_SCORE', 'TRANSACTION_AMOUNT', 'EMI_TENURE'], outputCol='rfm_amt_emi')\n","df2 = va1.transform(df)\n","ss = StandardScaler(withMean=True, inputCol='rfm_amt_emi', outputCol='rfm_amt_emi_scaled')\n","ssmodel = ss.fit(df2)\n","df2 = ssmodel.transform(df2)\n","print(ssmodel.mean, ssmodel.std)"]},{"cell_type":"markdown","metadata":{},"source":["#### Verify standard scaler does what is expected\n","\n","Scaling again should yield a mean of 0 and std dev of 1."]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"[5.0237103973310715e-14,4.672222895032996e-14,4.715910821573299e-14] [1.0000000000000004,0.9999999999999951,0.9999999999999921]\n"}],"source":["ss2 = StandardScaler(inputCol=\"rfm_amt_emi_scaled\", outputCol=\"scaled_twice\")\n","ssmodel2 = ss2.fit(df2)\n","print(ssmodel2.mean, ssmodel2.std)"]},{"cell_type":"markdown","metadata":{},"source":["The work on categorical features. First use `StringIndexer` to convert to categorical. Then for cases with more than 2 categories, use one-hot encoding. Also start assembling feature names."]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"data":{"text/plain":"[Row(ACCOUNT_AGE='up to 1 YR', ACCOUNT_TYPE='above 1000 K USD', CREDIT_HISTORY='EXISTING CREDITS PAID BACK', EMI_TENURE=105, HAS_CO_APPLICANT='NO', HAS_GUARANTOR='YES', IS_DEFAULT='No', IS_STATE_BORDER='YES', IS_URBAN='YES', NUMBER_CREDITS=0, OTHER_INSTALMENT_PLAN='NO', OWN_CAR='YES', OWN_REAL_ESTATE='YES', OWN_RESIDENCE='YES', RFM_SCORE=4, SHIP_INTERNATIONAL='YES', STATE='CT', TRANSACTION_AMOUNT=25788, TRANSACTION_CATEGORY='EDUCATION', rfm_amt_emi=DenseVector([4.0, 25788.0, 105.0]), rfm_amt_emi_scaled=DenseVector([0.7892, -0.2587, -0.248]), label=0.0, account_age_ind=0.0, account_type_ind=0.0, credit_history_ind=0.0, has_co_applicant_ind=0.0, has_guarantor_ind=1.0, is_state_border_ind=1.0, is_urban_ind=0.0, other_instalment_plan_ind=0.0, own_car_ind=0.0, own_real_estate_ind=1.0, own_residence_ind=1.0, ship_international_ind=0.0, state_ind=0.0, transaction_category_ind=2.0, account_age_vec=SparseVector(4, {0: 1.0}), account_type_vec=SparseVector(4, {0: 1.0}), credit_history_vec=SparseVector(4, {0: 1.0}), transaction_vec=SparseVector(6, {2: 1.0}), state_vec=SparseVector(3, {0: 1.0}), features=SparseVector(33, {0: 1.0, 4: 1.0, 8: 1.0, 13: 1.0, 14: 1.0, 18: 1.0, 19: 1.0, 21: 1.0, 26: 1.0, 30: 0.7892, 31: -0.2587, 32: -0.248}))]"},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["indexer = StringIndexer(inputCol=\"IS_DEFAULT\", outputCol=\"label\")\n","ndf = indexer.fit(df2).transform(df2)\n","names = [\"ACCOUNT_AGE\", \"ACCOUNT_TYPE\", \"CREDIT_HISTORY\", \n","         \"HAS_CO_APPLICANT\", \"HAS_GUARANTOR\", \"IS_STATE_BORDER\", \n","         \"IS_URBAN\", \"OTHER_INSTALMENT_PLAN\",\"OWN_CAR\", \n","         \"OWN_REAL_ESTATE\", \"OWN_RESIDENCE\", \"SHIP_INTERNATIONAL\", \n","         \"STATE\", \"TRANSACTION_CATEGORY\"]\n","feat_names = []\n","feat_base = []\n","for ind, name in enumerate(names):\n","    inpcol = name\n","    oupcol = name.lower() + \"_ind\"\n","    indexer = StringIndexer(inputCol = inpcol, outputCol = oupcol)\n","    model = indexer.fit(ndf)\n","    ndf = model.transform(ndf)\n","    this_labels = model.labels[:-1]\n","    [feat_names.append(inpcol + \"(\" + a_label + \")\") for a_label in this_labels]\n","    feat_base.append(inpcol + \"(\" + model.labels[-1] + \")\")\n","    # last category is not included by default\n","    # see docs here: https://spark.apache.org/docs/2.3.0/api/python/pyspark.ml.html#pyspark.ml.feature.OneHotEncoderEstimator\n","encoder = OneHotEncoder(inputCol=\"account_age_ind\", outputCol=\"account_age_vec\")\n","ndf = encoder.transform(ndf)\n","encoder = OneHotEncoder(inputCol=\"account_type_ind\", outputCol=\"account_type_vec\")\n","ndf = encoder.transform(ndf)\n","encoder = OneHotEncoder(inputCol=\"credit_history_ind\", outputCol=\"credit_history_vec\")\n","ndf = encoder.transform(ndf)\n","encoder = OneHotEncoder(inputCol=\"transaction_category_ind\", outputCol=\"transaction_vec\")\n","ndf = encoder.transform(ndf)\n","encoder = OneHotEncoder(inputCol=\"state_ind\", outputCol=\"state_vec\")\n","ndf = encoder.transform(ndf)\n","\n","assembler = VectorAssembler(\n","    inputCols=[\"account_age_vec\", \"account_type_vec\", \"credit_history_vec\",\n","               \"has_co_applicant_ind\", \"has_guarantor_ind\", \"is_state_border_ind\", \n","               \"is_urban_ind\", \"other_instalment_plan_ind\", \"own_car_ind\", \n","               \"own_real_estate_ind\", \"own_residence_ind\", \"ship_international_ind\",\n","               \"state_vec\", \"transaction_vec\",\n","               \"rfm_amt_emi_scaled\"],\n","    outputCol=\"features\")\n","\n","ndf = assembler.transform(ndf)\n","ndf.take(1)"]},{"cell_type":"markdown","metadata":{},"source":["## Get feature names\n","\n","- For the entire input vector\n","- For the \"base\" case, where all categorical indices are 0 and numerical features are set to mean values."]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"['ACCOUNT_AGE(1 to 4 YRS)', 'ACCOUNT_TYPE(500 to 1000 K USD)', 'CREDIT_HISTORY(CRITICAL ACCOUNT)', 'HAS_CO_APPLICANT(YES)', 'HAS_GUARANTOR(YES)', 'IS_STATE_BORDER(YES)', 'IS_URBAN(NO)', 'OTHER_INSTALMENT_PLAN(YES)', 'OWN_CAR(NO)', 'OWN_REAL_ESTATE(YES)', 'OWN_RESIDENCE(YES)', 'SHIP_INTERNATIONAL(NO)', 'STATE(NJ)', 'TRANSACTION_CATEGORY(OTHER)', 'rfm = 3.55', 'transaction amount = 26980.0', 'emi tenure = 109.68']\n"}],"source":["feat_names_full = feat_names + [\"scaled rfm\", \"scaled transaction amount\", \"scaled emi tenure\"]\n","feat_base_full = feat_base + [\"rfm = \" + str(round(ssmodel.mean[0], 2)), \n","                              \"transaction amount = \" + str(round(ssmodel.mean[1], 2)),\n","                              \"emi tenure = \" + str(round(ssmodel.mean[2], 2))]\n","print(feat_base_full)"]},{"cell_type":"markdown","metadata":{},"source":["### Save features and transformed data"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["np.savez(\"feat_names.npz\", feat_base_full=feat_base_full, feat_names_full=feat_names_full,\n","        mean=ssmodel.mean.toArray(), std=ssmodel.std.toArray())"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["ndf.select(\"label\", \"features\").write.mode('overwrite').save(\"credit_feats.parquet\")"]}],"metadata":{"kernelspec":{"name":"python36","display_name":"Python 3.6 with Spark","language":"python3"},"language_info":{"mimetype":"text/x-python","nbconvert_exporter":"python","name":"python","pygments_lexer":"ipython3","version":"3.6.8","file_extension":".py","codemirror_mode":{"version":3,"name":"ipython"}}},"nbformat":4,"nbformat_minor":1}